{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocaburary_size = 8000\n",
    "unknown_token = 'UNKNOWN_TOKEN'\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 79170 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Read the csv file(open in text mode in python3) and append SENTENCE_START and SENTENCE_END tokens\n",
    "with open('reddit-comments-2015-08.csv', 'r') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    header = next(reader)\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print('Parsed %d sentences.' % (len(sentences)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized into 1716189 words\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "print('Tokenized into %d words' % (np.sum([len(ts) for ts in tokenized_sentences])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  65752 unique word tokens.\n"
     ]
    }
   ],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print('Found  %d unique word tokens.' % len(word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocaburary size:  8000\n",
      "The least frequent word in our vocaburary is questioning and appeared 10 times.\n"
     ]
    }
   ],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocaburary_size - 1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w, i) for (i, w) in enumerate(index_to_word)])\n",
    "print('Using vocaburary size: ', vocaburary_size)\n",
    "print('The least frequent word in our vocaburary is', vocab[-1][0], 'and appeared', vocab[-1][-1], 'times.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences: SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END\n",
      "Example sentences after pre-processing: SENTENCE_START i joined a new league this year and they have different scoring rules than i 'm used to . SENTENCE_END\n"
     ]
    }
   ],
   "source": [
    "# Replace all words not in our vocaburary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "print('Example sentences:', sentences[0])\n",
    "print('Example sentences after pre-processing:', ' '.join(tokenized_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences: ['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']\n",
      "Example X_train: [1, 6, 3528, 7, 155, 792, 25, 223, 8, 32, 20, 203, 5072, 349, 91, 6, 66, 207, 5, 2]\n",
      "Example y_train: [6, 3528, 7, 155, 792, 25, 223, 8, 32, 20, 203, 5072, 349, 91, 6, 66, 207, 5, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# Create training set\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "print('Example sentences:', tokenized_sentences[0])\n",
    "print('Example X_train:', X_train[0])\n",
    "print('Example y_train:', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN paramters\n",
    "# x_t = [8000,]   input \n",
    "# o_t = [8000,]  output\n",
    "# s_t = [100,]     hidden state\n",
    "# U = [100, 8000]\n",
    "# V = [8000,100]\n",
    "# W = [100, 100]\n",
    "#\n",
    "# s_t = tanh(U * x_t + W * s_t-1)  \n",
    "# o_t = softmax(V * s_t)\n",
    "#\n",
    "# H(hidden size) = 100, C(vocaburary size) = 8000\n",
    "# total number of parametrs : U + V + W = 100 * 8000 + 8000 * 100 + 100 * 100 = 2HC + H^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNnumpy:\n",
    "    def __init__(self, word_dim, hidden_dim = 100, bptt_truncate = 4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network paramters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, hidden_dim))\n",
    "        \n",
    "    def forward_prop(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "        # we add one additional element for the initial hidden, which we set to 0.\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # for each time step\n",
    "        for t in np.arange(T):\n",
    "            # x_t is one hot vector \n",
    "            s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        o, s = self.forward_prop(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # for each sentence\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_prop(x[i])                           \n",
    "            len_sentence = len(y[i])\n",
    "            correct_word_index = y[i]\n",
    "            # we only care about our predictions of the correct words\n",
    "            correct_word_predictions = o[np.arange(len_sentence), correct_word_index]\n",
    "            # Add to the loss \n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        # Number of words in our text\n",
    "        N = np.sum([len(y_i) for y_i in y])\n",
    "        L = self.calculate_total_loss(x, y)\n",
    "        return L / N \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 8000)\n",
      "[[ 0.00012495  0.00012501  0.00012511 ...,  0.00012496  0.00012499\n",
      "   0.00012495]\n",
      " [ 0.00012504  0.00012506  0.00012495 ...,  0.00012499  0.00012496\n",
      "   0.00012495]\n",
      " [ 0.00012489  0.00012502  0.00012499 ...,  0.00012498  0.00012509\n",
      "   0.00012505]\n",
      " ..., \n",
      " [ 0.00012504  0.000125    0.00012498 ...,  0.00012499  0.00012492\n",
      "   0.00012494]\n",
      " [ 0.00012501  0.00012494  0.00012499 ...,  0.00012496  0.00012502\n",
      "   0.000125  ]\n",
      " [ 0.00012497  0.00012498  0.00012499 ...,  0.00012501  0.00012503\n",
      "   0.00012508]]\n",
      "(45,)\n",
      "[3989 7015 2594 2133 5068 6601 6559  415 2212 6601 1581 3106 6333 5898 5738\n",
      " 1712 6548 6164 7551 5898 1835 5145 5617 4665 6336 4821  831 4951 5207 1835\n",
      " 3850 4048 5301 5898 4864 2182 1390 5898 3848 6821 4437 1528 2390 5027 6862]\n",
      "Expected loss for random predictions: 8.98719682066\n",
      "Actual loss: 8.98720245127\n"
     ]
    }
   ],
   "source": [
    "# Try an implementation of RNNnumpy class\n",
    "np.random.seed(10)\n",
    "model = RNNnumpy(vocaburary_size)\n",
    "o, s = model.forward_prop(X_train[10])\n",
    "print(o.shape)\n",
    "print(o)\n",
    "preds = model.predict(X_train[10])\n",
    "print(preds.shape)\n",
    "print(preds)\n",
    "\n",
    "# calculate the loss\n",
    "print('Expected loss for random predictions:', np.log(vocaburary_size))\n",
    "print('Actual loss:', model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
